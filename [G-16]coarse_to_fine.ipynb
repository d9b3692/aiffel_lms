{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "buried-bubble",
   "metadata": {},
   "source": [
    "# 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aggregate-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib, cv2\n",
    "\n",
    "LFW_PATH = os.getenv('HOME') + '/lfw'\n",
    "DATA_PATH = os.getenv('HOME') + '/aiffel/coarse_to_fine/lfw_data'\n",
    "TRAIN_IMAGE_PATH = os.path.join(DATA_PATH, 'train', 'input')\n",
    "TRAIN_LABEL_PATH = os.path.join(DATA_PATH, 'train', 'label')\n",
    "VALID_IMAGE_PATH = os.path.join(DATA_PATH, 'val', 'input')\n",
    "VALID_LABEL_PATH = os.path.join(DATA_PATH, 'val', 'label')\n",
    "\n",
    "IMAGE_SHAPE = (80, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-briefs",
   "metadata": {},
   "source": [
    "# MEAN-SHIFT를 이용하여 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "horizontal-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_face_files(directory):\n",
    "    files = []\n",
    "    for file in os.listdir(directory):\n",
    "        current = os.path.join(directory, file)\n",
    "        if os.path.isdir(current):\n",
    "            files.extend(search_face_files(current))\n",
    "        else:\n",
    "            ext = os.path.splitext(current)[-1]\n",
    "            if ext == '.jpg' or ext == '.png':\n",
    "                files.append(current)\n",
    "    return files\n",
    "\n",
    "def eye_crop(image, landmark):\n",
    "    # dlib eye landmark: 36~41 (6), 42~47 (6)\n",
    "    left_eye_points = np.array(landmark[36:42])\n",
    "    right_eye_points = np.array(landmark[42:48])\n",
    "\n",
    "    left_top_left = left_eye_points.min(axis=0)\n",
    "    left_bottom_right = left_eye_points.max(axis=0)\n",
    "    right_top_left = right_eye_points.min(axis=0)\n",
    "    right_bottom_right = right_eye_points.max(axis=0)\n",
    "\n",
    "    left_eye_size = left_bottom_right - left_top_left\n",
    "    right_eye_size = right_bottom_right - right_top_left\n",
    "\n",
    "    ### if eye size is small\n",
    "    if left_eye_size[1] < 5:\n",
    "        margin = 1\n",
    "    else:\n",
    "        margin = 2\n",
    "\n",
    "    img_left_eye = image[left_top_left[1] - margin:left_bottom_right[1] + margin,\n",
    "                   left_top_left[0] - margin:left_bottom_right[0] + margin]\n",
    "    img_right_eye = image[right_top_left[1] - margin:right_bottom_right[1] + margin,\n",
    "                    right_top_left[0] - margin:right_bottom_right[0] + margin]\n",
    "\n",
    "    return [img_left_eye, img_right_eye]\n",
    "\n",
    "\n",
    "# 눈 이미지에서 중심을 찾는 함수\n",
    "def findCenterPoint(gray_eye, str_direction='left'):\n",
    "    filtered_eye = cv2.bilateralFilter(gray_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "    filtered_eye = cv2.bilateralFilter(filtered_eye, 7, 75, 75)\n",
    "\n",
    "    # 2D images -> 1D signals\n",
    "    row_sum = 255 - np.sum(filtered_eye, axis=0) // gray_eye.shape[0]\n",
    "    col_sum = 255 - np.sum(filtered_eye, axis=1) // gray_eye.shape[1]\n",
    "\n",
    "    # normalization & stabilization\n",
    "    def vector_normalization(vector):\n",
    "        vector = vector.astype(np.float32)\n",
    "        vector = (vector - vector.min()) / (vector.max() - vector.min() + 1e-6) * 255\n",
    "        vector = vector.astype(np.uint8)\n",
    "        vector = cv2.blur(vector, (5, 1)).reshape((vector.shape[0],))\n",
    "        vector = cv2.blur(vector, (5, 1)).reshape((vector.shape[0],))\n",
    "        return vector\n",
    "\n",
    "    row_sum = vector_normalization(row_sum)\n",
    "    col_sum = vector_normalization(col_sum)\n",
    "\n",
    "    def findOptimalCenter(gray_eye, vector, str_axis='x'):\n",
    "        axis = 1 if str_axis == 'x' else 0\n",
    "        center_from_start = np.argmax(vector)\n",
    "        center_from_end = gray_eye.shape[axis] - 1 - np.argmax(np.flip(vector, axis=0))\n",
    "        return (center_from_end + center_from_start) // 2\n",
    "\n",
    "\n",
    "    # x 축 center 를 찾는 알고리즘을 mean shift 로 대체합니다.\n",
    "    # center_x = findOptimalCenter(gray_eye, row_sum, 'x')\n",
    "    center_y = findOptimalCenter(gray_eye, col_sum, 'y')\n",
    "\n",
    "    # 수정된 부분\n",
    "    inv_eye = (255 - filtered_eye).astype(np.float32)\n",
    "    inv_eye = (255 * (inv_eye - inv_eye.min()) / (inv_eye.max() - inv_eye.min())).astype(np.uint8)\n",
    "\n",
    "    resized_inv_eye = cv2.resize(inv_eye, (inv_eye.shape[1] // 3, inv_eye.shape[0] // 3))\n",
    "    init_point = np.unravel_index(np.argmax(resized_inv_eye), resized_inv_eye.shape)\n",
    "\n",
    "    x_candidate = init_point[1] * 3 + 1\n",
    "    for idx in range(10):\n",
    "        temp_sum = row_sum[x_candidate - 2:x_candidate + 3].sum()\n",
    "        if temp_sum == 0:\n",
    "            break\n",
    "        normalized_row_sum_part = row_sum[x_candidate - 2:x_candidate + 3].astype(np.float32) // temp_sum\n",
    "        moving_factor = normalized_row_sum_part[3:5].sum() - normalized_row_sum_part[0:2].sum()\n",
    "        if moving_factor > 0.0:\n",
    "            x_candidate += 1\n",
    "        elif moving_factor < 0.0:\n",
    "            x_candidate -= 1\n",
    "\n",
    "    center_x = x_candidate\n",
    "\n",
    "    if center_x >= gray_eye.shape[1] - 2 or center_x <= 2:\n",
    "        center_x = -1\n",
    "    elif center_y >= gray_eye.shape[0] - 1 or center_y <= 1:\n",
    "        center_y = -1\n",
    "\n",
    "    return [center_x, center_y]\n",
    "\n",
    "# 눈동자 검출 wrapper 함수\n",
    "def detectPupil(left_eye, right_eye):\n",
    "    gray_left_eye = cv2.cvtColor(left_eye, cv2.COLOR_BGR2GRAY)\n",
    "    gray_right_eye = cv2.cvtColor(right_eye, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    left_center_x, left_center_y = findCenterPoint(gray_left_eye, 'left')\n",
    "    right_center_x, right_center_y = findCenterPoint(gray_right_eye, 'right')\n",
    "\n",
    "    return left_center_x, left_center_y, right_center_x, right_center_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "revised-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_eyes_from_files(files):\n",
    "    detector_hog = dlib.get_frontal_face_detector()  # detector 선언\n",
    "    landmark_file = os.getenv('HOME') + '/aiffel/coarse_to_fine/models/shape_predictor_68_face_landmarks.dat'\n",
    "    landmark_predictor = dlib.shape_predictor(landmark_file)\n",
    "\n",
    "    for idx, img_file in enumerate(files):\n",
    "        img = cv2.imread(img_file)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if idx % 10 == 0:  # for validation\n",
    "            input_dir = os.path.join(VALID_IMAGE_PATH, 'img')\n",
    "            label_dir = os.path.join(VALID_LABEL_PATH, 'mask')\n",
    "        else:    # for train\n",
    "            input_dir = os.path.join(TRAIN_IMAGE_PATH, 'img')\n",
    "            label_dir = os.path.join(TRAIN_LABEL_PATH, 'mask')\n",
    "\n",
    "        left_eye_img_path = os.path.join(input_dir, ('eye_%06d_l.png' % idx))\n",
    "        right_eye_img_path = os.path.join(input_dir, ('eye_%06d_r.png' % idx))\n",
    "        left_eye_label_path = os.path.join(label_dir, ('eye_%06d_l.png' % idx))\n",
    "        right_eye_label_path = os.path.join(label_dir, ('eye_%06d_r.png' % idx))\n",
    "\n",
    "        dlib_rects = detector_hog(img_rgb, 1)  # (image, num of img pyramid)\n",
    "\n",
    "        list_landmarks = []\n",
    "        for dlib_rect in dlib_rects:\n",
    "            points = landmark_predictor(img_rgb, dlib_rect)\n",
    "            list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "            list_landmarks.append(list_points)\n",
    "        if not list_landmarks:\n",
    "            continue\n",
    "        landmark = list_landmarks[0]\n",
    "\n",
    "        # 눈 이미지 crop\n",
    "        img_left_eye, img_right_eye = eye_crop(img, landmark)\n",
    "        if not np.any(img_left_eye) or not np.any(img_right_eye):\n",
    "            continue\n",
    "        # 눈동자 중심 좌표 출력\n",
    "        left_center_x, left_center_y, right_center_x, right_center_y = detectPupil(img_left_eye, img_right_eye)\n",
    "\n",
    "        left_left_x = landmark[36][0]\n",
    "        left_left_y = landmark[36][1]\n",
    "        left_right_x = landmark[39][0]\n",
    "        left_right_y = landmark[39][1]\n",
    "        right_left_x = landmark[42][0]\n",
    "        right_left_y = landmark[42][1]\n",
    "        right_right_x = landmark[45][0]\n",
    "        right_right_y = landmark[45][1]\n",
    "\n",
    "        # left eye 이미지 출력\n",
    "        cv2.imwrite(left_eye_img_path, img_left_eye)\n",
    "\n",
    "        img_label_left = cv2.circle(np.zeros_like(img_left_eye), (left_left_x, left_left_y), 3, (1), -1) \n",
    "        img_label_left = cv2.circle(img_label_left, (left_right_x, left_right_y), 3, (2), -1) \n",
    "        img_label_left = cv2.circle(img_label_left, (left_center_x, left_center_y), 3, (3), -1) \n",
    "        cv2.imwrite(left_eye_label_path, img_label_left)\n",
    "\n",
    "        # right eye 이미지 출력\n",
    "        cv2.imwrite(right_eye_img_path, img_right_eye)\n",
    "\n",
    "        img_label_right = cv2.circle(np.zeros_like(img_right_eye), (right_left_x, right_left_y), 3, (1), -1)\n",
    "        img_label_right = cv2.circle(img_label_right, (right_right_x, right_right_y), 3, (2), -1)\n",
    "        img_label_right = cv2.circle(img_label_right, (right_center_x, right_center_y), 3, (3), -1)\n",
    "        cv2.imwrite(right_eye_label_path, img_label_right)\n",
    "\n",
    "        if idx % 1000 == 0:\n",
    "            print('%d image OK..' % idx)\n",
    "\n",
    "    print('Finished!!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-complement",
   "metadata": {},
   "source": [
    "# 데이터 생성기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "female-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_generation(train_generator, label_generator):\n",
    "    h, w = train_generator.target_size\n",
    "    for images, labels in zip(train_generator, label_generator):\n",
    "        images /= 255.\n",
    "        images = images[..., ::-1] # rgb to bgr\n",
    "\n",
    "        list_point_labels = []\n",
    "        for img, label in zip(images, labels):\n",
    "\n",
    "            eye_ls = np.where(label==1) # leftside\n",
    "            eye_rs = np.where(label==2) # rightside\n",
    "            eye_center = np.where(label==3)\n",
    "\n",
    "            lx, ly = [eye_ls[1].mean(), eye_ls[0].mean()]\n",
    "            rx, ry = [eye_rs[1].mean(), eye_rs[0].mean()]\n",
    "            cx, cy = [eye_center[1].mean(), eye_center[0].mean()]\n",
    "\n",
    "            if len(eye_ls[0])==0 or len(eye_ls[1])==0:\n",
    "                lx, ly = [0, 0]\n",
    "            if len(eye_rs[0])==0 or len(eye_rs[1])==0:\n",
    "                rx, ry = [w, h]\n",
    "            if len(eye_center[0])==0 or len(eye_center[1])==0:\n",
    "                cx, cy = [0, 0]\n",
    "\n",
    "            np_point_label = np.array([lx/w,ly/h,rx/w,ry/h,cx/w,cy/h], dtype=np.float32)\n",
    "\n",
    "            list_point_labels.append(np_point_label)\n",
    "        np_point_labels = np.array(list_point_labels)\n",
    "        yield (images, np_point_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-blocking",
   "metadata": {},
   "source": [
    "# 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "animated-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_step_decay(epoch):\n",
    "    init_lr = 0.0005 #self.flag.initial_learning_rate\n",
    "    lr_decay = 0.5 #self.flag.learning_rate_decay_factor\n",
    "    epoch_per_decay = 2 #self.flag.epoch_per_decay\n",
    "    lrate = init_lr * math.pow(lr_decay, math.floor((1+epoch)/epoch_per_decay))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-groove",
   "metadata": {},
   "source": [
    "# 메인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-enzyme",
   "metadata": {},
   "source": [
    "## 원본 lfw데이터로부터 데이터를 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "portable-advocate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 image OK..\n",
      "1000 image OK..\n",
      "2000 image OK..\n",
      "3000 image OK..\n",
      "4000 image OK..\n",
      "5000 image OK..\n",
      "6000 image OK..\n",
      "7000 image OK..\n",
      "8000 image OK..\n",
      "9000 image OK..\n",
      "10000 image OK..\n",
      "11000 image OK..\n",
      "12000 image OK..\n",
      "13000 image OK..\n",
      "Finished!!\n"
     ]
    }
   ],
   "source": [
    "files = search_face_files(LFW_PATH)\n",
    "crop_eyes_from_files(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-organization",
   "metadata": {},
   "source": [
    "## 정제된 데이터 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "universal-contamination",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23712 images belonging to 1 classes.\n",
      "Found 23712 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "label_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "image_data = image_generator.flow_from_directory(IMAGE_PATH, class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "label_data = label_generator.flow_from_directory(LABEL_PATH, class_mode=None, target_size=IMAGE_SHAPE, batch_size=32)\n",
    "user_train_generator = user_generation(image_data, label_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-shepherd",
   "metadata": {},
   "source": [
    "## 모델 준비 & 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "transsexual-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer_2 (KerasLayer)   (None, 2048)              23564800  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 12294     \n",
      "=================================================================\n",
      "Total params: 23,577,094\n",
      "Trainable params: 12,294\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4\"\n",
    "feature_extractor_layer = hub.KerasLayer(feature_extractor_url, input_shape=(80,120,3))\n",
    "model = tf.keras.Sequential([\n",
    "    feature_extractor_layer,\n",
    "    tf.keras.layers.Dense(6, activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "  optimizer=tf.keras.optimizers.Adam(),\n",
    "  loss='mse',\n",
    "  metrics=['mae']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "undefined-relaxation",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23712 32 741\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ssac5/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: Mean of empty slice.\n",
      "  \n",
      "/home/ssac5/anaconda3/envs/aiffel/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/ssac5/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: Mean of empty slice.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741/741 [==============================] - 111s 150ms/step - loss: 0.0197 - mae: 0.0687\n",
      "Epoch 2/10\n",
      "741/741 [==============================] - 111s 149ms/step - loss: 0.0144 - mae: 0.0561\n",
      "Epoch 3/10\n",
      "741/741 [==============================] - 112s 152ms/step - loss: 0.0137 - mae: 0.0544\n",
      "Epoch 4/10\n",
      "741/741 [==============================] - 112s 151ms/step - loss: 0.0133 - mae: 0.0534\n",
      "Epoch 5/10\n",
      "741/741 [==============================] - 111s 149ms/step - loss: 0.0131 - mae: 0.0528\n",
      "Epoch 6/10\n",
      "741/741 [==============================] - 111s 150ms/step - loss: 0.0130 - mae: 0.0525\n",
      "Epoch 7/10\n",
      "741/741 [==============================] - 111s 150ms/step - loss: 0.0129 - mae: 0.0522\n",
      "Epoch 8/10\n",
      "741/741 [==============================] - 114s 153ms/step - loss: 0.0129 - mae: 0.0521\n",
      "Epoch 9/10\n",
      "741/741 [==============================] - 112s 152ms/step - loss: 0.0128 - mae: 0.0520\n",
      "Epoch 10/10\n",
      "741/741 [==============================] - 112s 152ms/step - loss: 0.0128 - mae: 0.0519\n"
     ]
    }
   ],
   "source": [
    "learning_rate = tf.keras.callbacks.LearningRateScheduler(lr_step_decay)\n",
    "history = model.fit(\n",
    "    user_train_generator,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=image_data.samples//image_data.batch_size,\n",
    "    callbacks = [learning_rate]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "assisted-oakland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'mae', 'lr'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb7d6344890>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-fraud",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-relay",
   "metadata": {},
   "source": [
    "dlib로부터 추출한 대략의 위치를 추정한 데이터를 가지고 학습을 시킨다면 모델이 제대로 학습될 수 없다.(dlib를 뛰어 넘을 수 없다)   \n",
    "이점은 mean_shift를 포함하여 모든 수학적인 방법을 동원해도 달라지지 않는다.   \n",
    "그러므로 데이터를 아무리 늘린다고 하여도 좋은 결과를 내놓는 모델을 만들 수 없다.   \n",
    "필요한 작업은 dlib를 이용하여 얻은 대략의 위치를 사람이 다시 재조정하는 일이다.   \n",
    "수작업으로 데이터를 하나하나 정제해야 좋은 모델을 학습시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-arrival",
   "metadata": {},
   "source": [
    "위 작업을 했다고 가정하고 좋은 모델을 얻었다고 가정한다고 하여도, 이 모델이 할 수 있는 작업은 ('눈' 그림, 눈동자의 위치)쌍을 얻어내는 것이다.   \n",
    "하지만 놀라는 눈을 붙이기 위해서는 '사람' 그림에서 눈동자 위치를 알아내는 작업이 필요하다.   \n",
    "결국 '사람'그림에서 '눈'위치를 얻은 후 모델에 입력하여야 할텐데, '사람'그림에서 '눈'위치를 얻기 위해서는 다시 dlib를 사용해야 한다.   \n",
    "따라서 딥러닝 모델만 가지고 놀라는 눈을 붙일 수 없게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-giant",
   "metadata": {},
   "source": [
    "모델만 이용하여 놀라는 눈을 붙이고 싶다면, 처음부터 ('사람'그림, 눈동자의 위치) 데이터를 가지고 모델을 학습시켜야 하고, 첫 단계부터 다시 수행되어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
