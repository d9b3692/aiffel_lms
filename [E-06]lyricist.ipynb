{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, re\n",
    "from pprint import pprint\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lyrics():\n",
    "    '''\n",
    "    파일에서 가사들을 불러옴, LMS동일\n",
    "    '''\n",
    "    dir_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "    result = []\n",
    "    for file_path in glob.glob(dir_path):\n",
    "        with open(file_path, 'r') as lyric_file:\n",
    "            sentences = lyric_file.read().splitlines()\n",
    "            result.extend(sentences)\n",
    "    return result\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    '''\n",
    "    문장들에서 특수 문자, 띄어쓰기 등을 보정한 후 <start>와 <end>를 문장 처음과 끝에 추가, LMS동일\n",
    "    '''\n",
    "    sentence = sentence.lower().strip()  \n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence\n",
    "\n",
    "def make_corpus(sentences):\n",
    "    corpus = []\n",
    "    print('원래 문장 TOP 10:')\n",
    "    pprint(sentences[:10])\n",
    "    print('\\n')\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) == 0: continue\n",
    "        if sentence[-1] == \":\": continue\n",
    "        corpus.append(preprocess_sentence(sentence))\n",
    "    print('특수 문자, 띄어쓰기 등을 보정한 문장 TOP 10:')\n",
    "    pprint(corpus[:10])\n",
    "    print('\\n')\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, num_words=12000):\n",
    "    '''\n",
    "    문장을 단어 단위로 끊고, 토큰으로 생성함, LMS와 거의 같음\n",
    "    \n",
    "    corpus    : 문장\n",
    "    num_words : 사용할 단어 수\n",
    "    \n",
    "    return    : 생성된 토큰 텐서, 토큰화에 사용된 맵\n",
    "    '''\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=num_words, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "    tensor = list(filter(lambda x: len(x) < 16, tensor)) # 토큰의 길이를 15개로 제한\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
    "\n",
    "    print('토큰화 + 패딩 후 문장 TOP 10:')\n",
    "    pprint(tensor[:10])\n",
    "    print('\\n')\n",
    "    print('토크나이저 TOP 10:')\n",
    "    for index in tokenizer.index_word:\n",
    "        print(tokenizer.index_word[index])\n",
    "        if index > 10:\n",
    "            break\n",
    "    print('\\n')\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_single_dataset(x, y, batch_size):\n",
    "    '''\n",
    "    한 개의 데이터셋을 생성. tf.data.Dataset\n",
    "    '''\n",
    "    size = len(x)\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).shuffle(size).batch(batch_size, drop_remainder=True)\n",
    "    \n",
    "def make_dataset(source, target, batch_size=256):\n",
    "    '''\n",
    "    훈련, 검증 데이터셋을 생성. tf.data.Dataset\n",
    "    \n",
    "    source : 문장의 시작\n",
    "    target : 문장의 다음\n",
    "    \n",
    "    return : 훈련 데이터셋, 검증 데이터셋\n",
    "    '''\n",
    "    train_x, test_x, train_y, test_y = train_test_split(source, target, test_size=0.2, shuffle=True)\n",
    "    print('훈련, 검증 데이터 분리 후 shape: ', train_x.shape, train_y.shape)\n",
    "    \n",
    "    train_dataset = make_single_dataset(train_x, train_y, batch_size)\n",
    "    val_dataset = make_single_dataset(test_x, test_y, batch_size)\n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "학습에 사용할 모델\n",
    "'''\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        # self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        \n",
    "        self.rnn_1 = tf.keras.layers.GRU(hidden_size, dropout=0.3, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.GRU(hidden_size, dropout=0.3, return_sequences=True)\n",
    "        \n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(sample, vocab_size=12001, embedding_size=256, hidden_size=1024):\n",
    "    '''\n",
    "    모델을 생성함\n",
    "    \n",
    "    sample         : 모델의 입력 텐서 차원을 알기 위한 샘플\n",
    "    vocab_size     : 단어의 가짓 수\n",
    "    embedding_size : 단어 임베딩의 깊이\n",
    "    hidden_size    : 모델 내부의 hidden layer의 수\n",
    "    \n",
    "    return         : 모델\n",
    "    '''\n",
    "    model = TextGenerator(vocab_size, embedding_size , hidden_size)\n",
    "    model(sample)\n",
    "    model.summary()\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.001,\n",
    "                                                              decay_steps=1950,\n",
    "                                                              decay_rate=0.5)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.00015)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    model.compile(loss=loss, optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    '''\n",
    "    문장이 어떻게 생성되는지 시험, LMS동일\n",
    "    \n",
    "    model         : 학습이 완료된 모델\n",
    "    tokenizer     : 문장을 토큰화 하는데 사용한 맵\n",
    "    init_sentence : 문장의 시작\n",
    "    max_len       : 최대 문장의 단어 수\n",
    "    \n",
    "    return        : 생성된 문장\n",
    "    '''\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 메인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 문장 TOP 10:\n",
      "['I saw you walking by his side heard you whisper all those lies',\n",
      " \"And I couldn't keep from crying\",\n",
      " 'You sang him love songs tenderly that should have been for you and me',\n",
      " \"And I couldn't keep from crying\",\n",
      " 'I saw his eyes drinking your charms while he held you in his arms',\n",
      " 'Him with all his wedding ways rules your heart now in my place',\n",
      " \"I stood and watched him steal a kiss from two lips I know I'll miss\",\n",
      " \"And I couldn't keep from crying I saw his eyes drinking your charms... I \"\n",
      " \"love that hair, long an' black\",\n",
      " \"Hangin' down to the middle of your back\",\n",
      " \"Don't cut it off whatever you do\"]\n",
      "\n",
      "\n",
      "특수 문자, 띄어쓰기 등을 보정한 문장 TOP 10:\n",
      "['<start> i saw you walking by his side heard you whisper all those lies <end>',\n",
      " '<start> and i couldn t keep from crying <end>',\n",
      " '<start> you sang him love songs tenderly that should have been for you and '\n",
      " 'me <end>',\n",
      " '<start> and i couldn t keep from crying <end>',\n",
      " '<start> i saw his eyes drinking your charms while he held you in his arms '\n",
      " '<end>',\n",
      " '<start> him with all his wedding ways rules your heart now in my place <end>',\n",
      " '<start> i stood and watched him steal a kiss from two lips i know i ll miss '\n",
      " '<end>',\n",
      " '<start> and i couldn t keep from crying i saw his eyes drinking your charms '\n",
      " '. . . i love that hair , long an black <end>',\n",
      " '<start> hangin down to the middle of your back <end>',\n",
      " '<start> don t cut it off whatever you do <end>']\n",
      "\n",
      "\n",
      "토큰화 + 패딩 후 문장 TOP 10:\n",
      "array([[   2,    5,  475,    7,  775,  122,  105,  300,  297,    7, 1710,\n",
      "          24,  423,  709,    3],\n",
      "       [   2,    8,    5,  601,   15,  129,   74,  878,    3,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2,    8,    5,  601,   15,  129,   74,  878,    3,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2,  142,   31,   24,  105, 1924,  889, 1162,   21,  139,   50,\n",
      "          14,   13,  248,    3],\n",
      "       [   2, 1691,   60,   10,    6,  881,   19,   21,   75,    3,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2,   37,   15,  399,   11,  119,  598,    7,   47,    3,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2, 1180,   11,   29,   28,    9, 3293,  118,    3,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2,  191,   16,  818,   28, 1763,   17,   16, 2347,    3,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2,    8,    9,    1,   10,   72,  125,  635,    3,    0,    0,\n",
      "           0,    0,    0,    0],\n",
      "       [   2,  133, 3047,   16,  145,   34,   40,   10,   47,    3,    0,\n",
      "           0,    0,    0,    0]], dtype=int32)\n",
      "\n",
      "\n",
      "토크나이저 TOP 10:\n",
      "<unk>\n",
      "<start>\n",
      "<end>\n",
      ",\n",
      "i\n",
      "the\n",
      "you\n",
      "and\n",
      "a\n",
      "to\n",
      "it\n",
      "\n",
      "\n",
      "훈련, 검증 데이터 분리 후 shape:  (124810, 14) (124810, 14)\n"
     ]
    }
   ],
   "source": [
    "num_words = 12000\n",
    "raw_sentences = read_lyrics()\n",
    "corpus = make_corpus(raw_sentences)\n",
    "tensor, tokenizer = tokenize(corpus, num_words)\n",
    "\n",
    "src_input = tensor[:,:-1]\n",
    "tgt_input = tensor[:,1:]\n",
    "\n",
    "train_dataset, val_dataset = make_dataset(src_input, tgt_input, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  12289024  \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  multiple                  12604800  \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  multiple                  15369600  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  19213601  \n",
      "=================================================================\n",
      "Total params: 59,477,025\n",
      "Trainable params: 59,477,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for src_sample, tgt_sample in train_dataset.take(1): break\n",
    "model = make_model(src_sample, vocab_size=num_words+1, embedding_size=1024, hidden_size=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3900/3900 [==============================] - 433s 111ms/step - loss: 3.0882 - val_loss: 2.8007\n",
      "Epoch 2/10\n",
      "3900/3900 [==============================] - 432s 111ms/step - loss: 2.6364 - val_loss: 2.5758\n",
      "Epoch 3/10\n",
      "3900/3900 [==============================] - 431s 111ms/step - loss: 2.3611 - val_loss: 2.4435\n",
      "Epoch 4/10\n",
      "3900/3900 [==============================] - 431s 111ms/step - loss: 2.1433 - val_loss: 2.3513\n",
      "Epoch 5/10\n",
      "3900/3900 [==============================] - 432s 111ms/step - loss: 1.9657 - val_loss: 2.2846\n",
      "Epoch 6/10\n",
      "3900/3900 [==============================] - 432s 111ms/step - loss: 1.8161 - val_loss: 2.2356\n",
      "Epoch 7/10\n",
      "3900/3900 [==============================] - 433s 111ms/step - loss: 1.6887 - val_loss: 2.2013\n",
      "Epoch 8/10\n",
      "3900/3900 [==============================] - 433s 111ms/step - loss: 1.5807 - val_loss: 2.1787\n",
      "Epoch 9/10\n",
      "3900/3900 [==============================] - 433s 111ms/step - loss: 1.4894 - val_loss: 2.1617\n",
      "Epoch 10/10\n",
      "3900/3900 [==============================] - 433s 111ms/step - loss: 1.4113 - val_loss: 2.1517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fba6c77de50>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love you <end> \n",
      "<start> you are the only thing that keeps me goin <end> \n",
      "<start> i am a god <end> \n",
      "<start> he is he is <end> \n",
      "<start> when i m with you , all i get is wild thoughts <end> \n"
     ]
    }
   ],
   "source": [
    "i_love = generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "print(i_love)\n",
    "\n",
    "you_are = generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=20)\n",
    "print(you_are)\n",
    "\n",
    "i_am = generate_text(model, tokenizer, init_sentence=\"<start> i am\", max_len=20)\n",
    "print(i_am)\n",
    "\n",
    "he_is = generate_text(model, tokenizer, init_sentence=\"<start> he is\", max_len=20)\n",
    "print(he_is)\n",
    "\n",
    "when_i = generate_text(model, tokenizer, init_sentence=\"<start> when i\", max_len=20)\n",
    "print(when_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결과들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 256, hidden 1024, LSTM 기본 두개 => loss: 2.2164 - val_loss: 2.5021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 256, hidden 1800, LSTM 기본 두개 => loss: 1.8641 - val_loss: 2.3710, 오버피팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 20000, batch 256, embedding 256, hidden 1024, LSTM 기본 두개 => loss: 2.3242 - val_loss: 2.6405"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 20000, batch 256, embedding 300, hidden 1024, LSTM 기본 두개 => loss: 2.3014 - val_loss: 2.6376"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 20000, batch 256, embedding 300, hidden 1200, LSTM 기본 두개 => loss: 2.2094 - val_loss: 2.5762"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 256, hidden 1024, LSTM 기본 두개 + Adam lr=0.005 => loss: 2.1292 - val_loss: 2.5862"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 256, hidden 1024, GRU 기본 두개 => loss: 1.4231 - val_loss: 2.2650, 오버피팅 심함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 256, hidden 1024, GRU + dropout=0.25 두개 => loss: 1.8833 - val_loss: 2.3803, 오버피팅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 256, hidden 1024, GRU + Adam lr=0.0005 두개 => loss: 2.1238 - val_loss: 2.4549"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 512, hidden 1024, GRU + Adam lr=0.0005 두개 => 1.8414 - val_loss: 2.3300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 256, embedding 512, hidden 1024, GRU + dropout=0.5 두개 => loss: 1.7902 - val_loss: 2.3288"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 20000, batch 256, embedding 512, hidden 1024, GRU 두개 => 1.9303 - val_loss: 2.4659"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 20000, batch 256, embedding 512, hidden 1024, GRU + dropout=0.5 두개 => loss: 2.1578 - val_loss: 2.5078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 20000, batch 128, embedding 512, hidden 1024, GRU 두개 => loss: 1.6931 - val_loss: 2.4475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 128, embedding 512, hidden 1024, GRU + dropout=0.5 두개 => loss: 1.7112 - val_loss: 2.3240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 128, embedding 256, hidden 1024, GRU + dropout=0.5 두개 => loss: 1.7756 - val_loss: 2.3797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 128, embedding 1024, hidden 1024, GRU + dropout=0.5 두개 => 1.6945 - val_loss: 2.3373"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 128, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0008 두개 => loss: 1.6680 - val_loss: 2.2846"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 128, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0006 두개 => loss: 1.6549 - val_loss: 2.2839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 64, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0006 두개 => loss: 1.6138 - val_loss: 2.2903"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 64, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0004 두개 => loss: 1.6461 - val_loss: 2.2577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 64, embedding 1024, hidden 1024, GRU + dropout=0.4 + lr=0.0004 두개 => loss: 1.5548 - val_loss: 2.2468"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 48, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0004 두개 => loss: 1.6135 - val_loss: 2.2499"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 32, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0003 두개 => loss: 1.6189 - val_loss: 2.2493"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 48, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0001 두개 => loss: 2.2202 - val_loss: 2.4396"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 32, embedding 1024, hidden 1024, GRU + dropout=0.5 + lr=0.0001 두개 => loss: 2.1082 - val_loss: 2.3981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 32, embedding 1024, hidden 1024, GRU + dropout=0.4 + lr=0.0001 두개 => loss: 2.0555 - val_loss: 2.3729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 32, embedding 1024, hidden 1024, GRU + dropout=0.3 + lr=0.0001 두개 => loss: 2.0155 - val_loss: 2.3624"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 32, embedding 1024, hidden 1600, GRU + dropout=0.3 + lr=0.0001 두개 => loss: 1.6642 - val_loss: 2.2220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어장 12000, batch 32, embedding 1024, hidden 1600, GRU + dropout=0.3 + lr=0.0002 두개 => loss: 1.2835 - val_loss: 2.1723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 64, embedding 512, hidden 1024, GRU + dropout=0.5 + lr=0.0004 두개 => loss: 1.7494 - val_loss: 2.3032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 48, embedding 512, hidden 1024, GRU + dropout=0.5 + lr=0.0004 두개 => loss: 1.6841 - val_loss: 2.2953"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어장 12000, batch 48, embedding 512, hidden 1024, GRU + dropout=0.4 + lr=0.0004 두개 => loss: 1.5877 - val_loss: 2.2765"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리와 학습 후 새로운 문장 생성 작업에는 LMS의 함수를 그대로 사용하였다.   \n",
    "길이가 긴 데이터의 처리는 keras의 pad_sequences함수에 옵션으로 부여하는 방법이 아닌 리스트에서 삭제하는 방식을 사용하였다.   \n",
    "전반적으로 어려움은 없었으나 val_loss를 줄여나가는 부분에서 오랜 시간이 걸렸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val_loss를 줄이기 위해서 초기에는 파라미터를 임의로 넣었으나, 낮아지는데 한계를 보였고, 각 파라미터들의 영향을 파악하는데 주력하기로 생각을 바꾸었다.   \n",
    "단어장 크기는 클수록 늦게 학습되는 경향이 있었고, embedding, hidden과 관련이 있었으므로 12000으로 고정하기로 결정하였다.   \n",
    "정해진 epoch에서 더 많은 학습을 진행하기 위해서는 batch 크기는 줄여야 했다.   \n",
    "LSTM은 학습 속도가 늦기 때문에 GRU를 사용하였고, GRU는 학습이 빠른 대신 과적합이 쉽게 일어났기 때문에 dropout을 적용하였다.   \n",
    "batch크기가 줄어든 만큼 learing rate도 줄여야 과적합을 줄일 수 있었다.   \n",
    "학습 속도와 과적합 사이에는 비례관계가 있었으므로 10 epoch안에 val_loss 2.2를 달성하기 위해 학습 속도가 적당히 빠르면서도 과적합이 발생하지 않는 파라미터를 찾아야 했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로 \"loss: 1.4113 - val_loss: 2.1517\"을 얻었고, 이로부터 몇가지 문장을 생성해보았다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성된 문장은 시작 단어에 따라 결과가 다른데, 일반적으로 노래 가사에 쓰이는 단어로 시작시켜야 그럴듯한 문장이 생성되는게 아닌가 생각이 든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
